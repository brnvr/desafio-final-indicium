{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# mssql_to_raw\n",
    "\n",
    "Moves data from SQL Server into Databricks Delta tables. Data will firstly be stored in the raw zone, where it is kept \"as-it-is\". You can change the raw zone catalog in Unity Catalog by setting the **raw_catalog_name** job parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "run_date = datetime.strptime(dbutils.widgets.get(\"run_date\"), '%Y-%m-%d')\n",
    "start_date = run_date - timedelta(days=1)\n",
    "secret_scope = dbutils.widgets.get(\"secret_scope\")\n",
    "ctr_catalog_name = dbutils.widgets.get(\"ctr_catalog_name\")\n",
    "raw_catalog_name = dbutils.widgets.get(\"raw_catalog_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lib.el import MSSqlDataLoader, SqlConnectionData\n",
    "from lib.logging import Log, Logger\n",
    "\n",
    "#Sets data necessary to connect to SQL Server\n",
    "connection_data = SqlConnectionData(\n",
    "    host = dbutils.widgets.get(\"mssql_host\"),\n",
    "    port = dbutils.widgets.get(\"mssql_port\"),\n",
    "    database = dbutils.widgets.get(\"mssql_database\"),\n",
    "    username = dbutils.secrets.get(secret_scope, \"mssql_username\"),\n",
    "    password = dbutils.secrets.get(secret_scope, \"mssql_password\")\n",
    ")\n",
    "\n",
    "#Selects the tables that will be ingested\n",
    "df_data_ingestion = (spark.read\n",
    "    .table(f\"{ctr_catalog_name}.loading.data_ingestion\")\n",
    "    .select(\n",
    "        \"schema_name\", \n",
    "        \"table_name\", \n",
    "        \"primary_key\", \n",
    "        \"filter\",\n",
    "        \"selected\",\n",
    "        \"partition_column\",\n",
    "        \"num_partitions\")\n",
    "    .where(\"active = true\")\n",
    ")\n",
    "\n",
    "logs = []\n",
    "processing_has_failed = False\n",
    "#Sets the filter to be used with incremental loads\n",
    "start_date_filter = f\"CONVERT(DATETIME, '{start_date.strftime('%Y-%m-%d')}', 120)\"\n",
    "\n",
    "for row in df_data_ingestion.collect():\n",
    "    schema_name = row[\"schema_name\"]\n",
    "    table_name = row[\"table_name\"]\n",
    "    partition_column = row[\"partition_column\"]\n",
    "    num_partitions = row[\"num_partitions\"]\n",
    "    primary_key = row[\"primary_key\"].replace(\" \", \"\").split(\",\")\n",
    "    selected = None if row[\"selected\"] is None else row[\"selected\"].replace(\" \", \"\").split(\",\")\n",
    "\n",
    "    data_loader = MSSqlDataLoader(\n",
    "        schema_name = schema_name, \n",
    "        table_name = table_name, \n",
    "        primary_key = primary_key,\n",
    "        connection_data = connection_data,\n",
    "        selected = selected,\n",
    "        num_partitions = num_partitions,\n",
    "        partition_column = partition_column\n",
    "    )\n",
    "\n",
    "    table_full_name = f\"{raw_catalog_name}.{schema_name}.{table_name}\"\n",
    "    \n",
    "    log = Log(\n",
    "        target_catalog_name = raw_catalog_name, \n",
    "        target_schema_name = schema_name,\n",
    "        target_table_name = table_name, \n",
    "        source_catalog_name = None, \n",
    "        source_schema_name = schema_name, \n",
    "        source_table_name = table_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        #If the table exists, loads it incrementally, else loads the full table\n",
    "        if spark.catalog.tableExists(table_full_name):\n",
    "            filter = row[\"filter\"].replace(\":start_date\", start_date_filter)\n",
    "        else:\n",
    "            filter = None\n",
    "\n",
    "        #Extracts data from SQL Server and loads it into the raw zone\n",
    "        data_loader.extract(filter).load_into(table_full_name)\n",
    "        log.movements = data_loader.df.count()\n",
    "    except Exception as e:\n",
    "        log.error = repr(e)\n",
    "        processing_has_failed = True\n",
    "        \n",
    "    logs.append(log)\n",
    "\n",
    "logger = Logger(f\"{ctr_catalog_name}.loading.data_ingestion_log\")\n",
    "\n",
    "logger.log(logs)\n",
    "\n",
    "if processing_has_failed:\n",
    "    raise RuntimeError(\"Processing of one or more tables has failed. Check the data ingestion log for further info.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
