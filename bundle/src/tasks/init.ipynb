{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Default notebook\n",
    "\n",
    "This default notebook is executed using Databricks Workflows as defined in resources/bundle.job.yml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ctr_catalog_name = dbutils.widgets.get(\"ctr_catalog_name\")\n",
    "raw_catalog_name = dbutils.widgets.get(\"raw_catalog_name\")\n",
    "stg_catalog_name = dbutils.widgets.get(\"stg_catalog_name\")\n",
    "managed_location = dbutils.widgets.get(\"managed_location\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    create schema if not exists {ctr_catalog_name}.loading\n",
    "    managed location '{managed_location}'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    create table if not exists {ctr_catalog_name}.loading.data_ingestion (\n",
    "        schema_name string, \n",
    "        table_name string,\n",
    "        primary_key string not null,\n",
    "        stg_primary_key string,\n",
    "        active boolean not null,\n",
    "        filter string,\n",
    "        selected string,\n",
    "        constraint data_ingestion_pk primary key (schema_name, table_name)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    create table if not exists {ctr_catalog_name}.loading.data_ingestion_log (\n",
    "        catalog_name string,\n",
    "        schema_name string, \n",
    "        table_name string,\n",
    "        ingestion_date timestamp,\n",
    "        movements integer not null,\n",
    "        error string,\n",
    "        constraint data_ingestion_log_pk primary key (catalog_name, schema_name, table_name, ingestion_date)\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schemas = (spark.read\n",
    "    .table(f\"{ctr_catalog_name}.loading.data_ingestion\")\n",
    "    .select(\"schema_name\")\n",
    "    .distinct()\n",
    "    .where(\"active = true\")\n",
    ")\n",
    "\n",
    "for schema_name in [row[\"schema_name\"] for row in df_schemas.collect()]:\n",
    "    spark.sql(f\"\"\"\n",
    "        create schema if not exists {raw_catalog_name}.{schema_name}\n",
    "        managed location '{managed_location}';    \n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        create schema if not exists {stg_catalog_name}.{schema_name}\n",
    "        managed location '{managed_location}';    \n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
