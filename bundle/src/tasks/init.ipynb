{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# init\n",
    "\n",
    "Initializes the job by creating necessary schemas and tables in Databricks (if non-existent), as well as populating loading.data_ingestion with initial entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ctr_catalog_name = dbutils.widgets.get(\"ctr_catalog_name\")\n",
    "raw_catalog_name = dbutils.widgets.get(\"raw_catalog_name\")\n",
    "stg_catalog_name = dbutils.widgets.get(\"stg_catalog_name\")\n",
    "managed_location = dbutils.widgets.get(\"managed_location\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {ctr_catalog_name}.loading\n",
    "    MANAGED LOCATION '{managed_location}'\n",
    "\"\"\")\n",
    "\n",
    "if not spark.catalog.tableExists(f\"{ctr_catalog_name}.loading.data_ingestion\"):\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {ctr_catalog_name}.loading.data_ingestion (\n",
    "            schema_name string, \n",
    "            table_name string,\n",
    "            primary_key string not null,\n",
    "            stg_primary_key string,\n",
    "            active boolean not null,\n",
    "            filter string,\n",
    "            selected string,\n",
    "            constraint data_ingestion_pk primary key (schema_name, table_name)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {ctr_catalog_name}.loading.data_ingestion (\n",
    "            schema_name, table_name, primary_key, selected, filter, active, stg_primary_key\n",
    "        ) VALUES \n",
    "            ('Sales', 'SalesTerritory', 'TerritoryID', NULL, 'ModifiedDate >= :start_date', TRUE, 'id'),\n",
    "            ('Sales', 'ShoppingCartItem', 'ShoppingCartItemID', NULL, 'ModifiedDate >= :start_date', TRUE, 'id'),\n",
    "            ('Sales', 'SpecialOffer', 'SpecialOfferID', NULL, 'ModifiedDate >= :start_date', TRUE, 'id'),\n",
    "            ('Sales', 'CurrencyRate', 'CurrencyRateID', NULL, 'ModifiedDate >= :start_date', TRUE, 'id'),\n",
    "            ('Sales', 'Customer', 'CustomerID', NULL, 'ModifiedDate >= :start_date', TRUE, 'id'),\n",
    "            ('Sales', 'SalesReason', 'SalesReasonID', NULL, 'ModifiedDate >= :start_date', TRUE, 'id'),\n",
    "            ('Sales', 'CreditCard', 'CreditCardID', NULL, 'ModifiedDate >= :start_date', TRUE, 'id'),\n",
    "            ('Sales', 'SalesTaxRate', 'SalesTaxRateID', NULL, 'ModifiedDate >= :start_date', TRUE, 'id'),\n",
    "            ('Sales', 'PersonCreditCard', 'BusinessEntityID, CreditCardID', NULL, 'ModifiedDate >= :start_date', TRUE, NULL),\n",
    "            ('Sales', 'SalesTerritoryHistory', 'BusinessEntityID, StartDate, TerritoryID', NULL, 'ModifiedDate >= :start_date', TRUE, NULL),\n",
    "            ('Sales', 'SpecialOfferProduct', 'SpecialOfferID, ProductID', NULL, 'ModifiedDate >= :start_date', TRUE, NULL),\n",
    "            ('Sales', 'CountryRegionCurrency', 'CountryRegionCode, CurrencyCode', NULL, 'ModifiedDate >= :start_date', TRUE, NULL),\n",
    "            ('Sales', 'Currency', 'CurrencyCode', NULL, 'ModifiedDate >= :start_date', TRUE, NULL),\n",
    "            ('Sales', 'SalesOrderDetail', 'SalesOrderID, SalesOrderDetailID', NULL, 'ModifiedDate >= :start_date', TRUE, NULL),\n",
    "            ('Sales', 'SalesOrderHeader', 'SalesOrderID', NULL, 'ModifiedDate >= :start_date', TRUE, NULL),\n",
    "            ('Sales', 'SalesOrderHeaderSalesReason', 'SalesOrderID, SalesReasonID', NULL, 'ModifiedDate >= :start_date', TRUE, NULL),\n",
    "            ('Sales', 'SalesPerson', 'BusinessEntityID', NULL, 'ModifiedDate >= :start_date', TRUE, NULL),\n",
    "            ('Sales', 'SalesPersonQuotaHistory', 'BusinessEntityID, QuotaDate', NULL, 'ModifiedDate >= :start_date', TRUE, NULL),\n",
    "            ('Sales', 'Store', 'BusinessEntityID', NULL, 'ModifiedDate >= :start_date', TRUE, NULL);\n",
    "    \"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {ctr_catalog_name}.loading.data_ingestion_log (\n",
    "        ingestion_date timestamp,\n",
    "        source_catalog_name string,\n",
    "        source_schema_name string not null, \n",
    "        source_table_name string not null,\n",
    "        target_catalog_name string,\n",
    "        target_schema_name string,\n",
    "        target_table_name string,\n",
    "        movements integer,\n",
    "        error string,\n",
    "        constraint data_ingestion_log_pk primary key (target_catalog_name, target_schema_name, target_table_name, ingestion_date)\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.naming import pascal_to_snake\n",
    "\n",
    "df_schemas = (spark.read\n",
    "    .table(f\"{ctr_catalog_name}.loading.data_ingestion\")\n",
    "    .select(\"schema_name\")\n",
    "    .distinct()\n",
    "    .where(\"active = true\")\n",
    ")\n",
    "\n",
    "for schema_name in [row[\"schema_name\"] for row in df_schemas.collect()]:\n",
    "    spark.sql(f\"\"\"\n",
    "        create schema if not exists {raw_catalog_name}.{schema_name}\n",
    "        managed location '{managed_location}';    \n",
    "    \"\"\")\n",
    "\n",
    "    # The schema in the staging zone will be created following snake_case naming convention, as this\n",
    "    # standard will be used for schemas, tables and columns from the staging zone onwards.\n",
    "    spark.sql(f\"\"\"\n",
    "        create schema if not exists {stg_catalog_name}.{pascal_to_snake(schema_name)}\n",
    "        managed location '{managed_location}';    \n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
