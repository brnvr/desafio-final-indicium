{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# raw_to_stg\n",
    "\n",
    "Moves data from the raw zone to the staging zone, and applies necessary transformations to tables. You can change the staging zone catalog in Unity Catalog by setting the **stg_catalog_name** job parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "run_date = datetime.strptime(dbutils.widgets.get(\"run_date\"), '%Y-%m-%d')\n",
    "start_date = run_date - timedelta(days=1)\n",
    "secret_scope = dbutils.widgets.get(\"secret_scope\")\n",
    "ctr_catalog_name = dbutils.widgets.get(\"ctr_catalog_name\")\n",
    "raw_catalog_name = dbutils.widgets.get(\"raw_catalog_name\")\n",
    "stg_catalog_name = dbutils.widgets.get(\"stg_catalog_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must define a dictionary called **transformations_dict** with the following structure:\n",
    "- The keys are tuples composed of the names of the source schema and table.\n",
    "- The values are lists of tuples, each tuple composed of the new table name (i.e. the one will be used in the staging zone) and a **transformation function**.\n",
    "\n",
    "The **transformation function** is a function which will transform the source table. It receives a spark DataFrame as argument, and returns the transformed DataFrame. If a table needs no transformation between the raw and staging zones, then the transformation function may be set to *None*.\n",
    "\n",
    "All tables will be renamed and transformed accordingly to the values specified in the dictionary. Tables which are not in the dictionary **will not be moved** to the staging zone.\n",
    "\n",
    "Besides the transformations defined in the transformation function, other transformation will be subsequentily be applied to the tables. \n",
    "1. If the entry relative to the source table in the **data_ingestion** control table has a **stg_primary_key** different than NULL, then the primary key will be renamed to the value of this column. Note that the number of items of the stg_primary_key (separated by comma) must be equal to the number of columns that compose the table primary key.\n",
    "2. All column names will conform to the snake_case naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.transformations import sales\n",
    "\n",
    "transformations_dict = {\n",
    "    #Source schema and table name                   #New table name and transformation function\n",
    "    (\"Sales\", \"CountryRegionCurrency\"):             [(\"country_region_currencies\", None)],\n",
    "    (\"Sales\", \"Currency\"):                          [(\"currencies\", None)],\n",
    "    (\"Sales\", \"CurrencyRate\"):                      [(\"currency_rates\", None)],\n",
    "    (\"Sales\", \"Customer\"):                          [(\"customers\", None)],\n",
    "    (\"Sales\", \"PersonCreditCard\"):                  [(\"person_credit_cards\", None)],\n",
    "    (\"Sales\", \"SalesOrderHeaderSalesReason\"):       [(\"order_header_sales_reasons\", None)],\n",
    "    (\"Sales\", \"SalesPersonQuotaHistory\"):           [(\"sales_person_quota_history\", None)],\n",
    "    (\"Sales\", \"SalesReason\"):                       [(\"sales_reasons\", None)],\n",
    "    (\"Sales\", \"SalesTaxRate\"):                      [(\"tax_rates\", None)],\n",
    "    (\"Sales\", \"SalesTerritory\"):                    [(\"territories\", None)],\n",
    "    (\"Sales\", \"SalesTerritoryHistory\"):             [(\"territory_history\", None)],\n",
    "    (\"Sales\", \"ShoppingCartItem\"):                  [(\"shopping_cart_items\", None)],\n",
    "    (\"Sales\", \"SpecialOfferProduct\"):               [(\"special_offer_products\", None)],\n",
    "    (\"Sales\", \"CreditCard\"):                        [(\"credits_cards\", sales.transform_credit_cards)],\n",
    "    (\"Sales\", \"SalesOrderDetail\"):                  [(\"order_details\", sales.transform_order_details)],\n",
    "    (\"Sales\", \"SalesOrderHeader\"):                  [(\"order_headers\", sales.transform_order_headers)],\n",
    "    (\"Sales\", \"SalesPerson\"):                       [(\"sales_people\", sales.transform_sales_people)],\n",
    "    (\"Sales\", \"SpecialOffer\"):                      [(\"special_offers\", sales.transform_special_offers)],\n",
    "    (\"Sales\", \"Store\"):                             [(\"stores\", sales.transform_stores), (\"store_demographics\", sales.transform_store_demographics)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from lib.el import DeltaDataLoader\n",
    "from lib.logging import Log, Logger\n",
    "from lib.naming import pascal_to_snake\n",
    "from lib.transformations import column_names_to_snakecase, column_names_renamed\n",
    "\n",
    "df_data_ingestion = (spark.read\n",
    "    .table(f\"{ctr_catalog_name}.loading.data_ingestion\")\n",
    "    .select(\n",
    "        \"schema_name\", \n",
    "        \"table_name\", \n",
    "        \"primary_key\", \n",
    "        \"stg_primary_key\",\n",
    "        \"filter\",\n",
    "        \"selected\")\n",
    "    .where(\"active = true\")\n",
    ")\n",
    "\n",
    "logs = []\n",
    "processing_has_failed = False\n",
    "start_date_filter = f\"TO_DATE('{start_date.strftime('%Y-%m-%d')}', 'yyyy-MM-dd')\"\n",
    "\n",
    "for row in df_data_ingestion.collect():\n",
    "    schema_name = row[\"schema_name\"]\n",
    "    table_name = row[\"table_name\"]\n",
    "    selected = None if row[\"selected\"] is None else row[\"selected\"].replace(\" \", \"\").split(\",\")\n",
    "    primary_key = [pascal_to_snake(pk) for pk in row[\"primary_key\"].replace(\" \", \"\").split(\",\")]\n",
    "\n",
    "    if row[\"stg_primary_key\"] is None:\n",
    "        stg_primary_key = None\n",
    "    else:\n",
    "        stg_primary_key = [pascal_to_snake(pk) for pk in row[\"stg_primary_key\"].replace(\" \", \"\").split(\",\")]\n",
    "\n",
    "    data_loader = DeltaDataLoader(\n",
    "        schema_name = schema_name,\n",
    "        table_name = table_name,\n",
    "        primary_key = primary_key if stg_primary_key is None else stg_primary_key,\n",
    "        selected = selected,\n",
    "        catalog_name = raw_catalog_name\n",
    "    )\n",
    "\n",
    "    schema_name_new = pascal_to_snake(schema_name)\n",
    "\n",
    "    if (schema_name, table_name) in transformations_dict:\n",
    "        transformations = transformations_dict[(schema_name, table_name)]\n",
    "\n",
    "        \n",
    "        for transformation in transformations:\n",
    "            table_name_new = transformation[0]\n",
    "\n",
    "            log = Log(\n",
    "                    target_catalog_name = stg_catalog_name, \n",
    "                    target_schema_name = schema_name_new,\n",
    "                    target_table_name = None, \n",
    "                    source_catalog_name = raw_catalog_name, \n",
    "                    source_schema_name = schema_name, \n",
    "                    source_table_name = table_name\n",
    "                )\n",
    "            \n",
    "            try: \n",
    "                table_full_name = f\"{stg_catalog_name}.{schema_name_new}.{table_name_new}\"\n",
    "                on_transform = (lambda df:df) if transformation[1] is None else transformation[1]\n",
    "\n",
    "                if spark.catalog.tableExists(table_full_name):\n",
    "                    filter = row[\"filter\"].replace(\":start_date\", start_date_filter)\n",
    "                else:\n",
    "                    filter = None   \n",
    "\n",
    "                data_loader \\\n",
    "                    .extract(filter) \\\n",
    "                    .apply(on_transform) \\\n",
    "                    .apply(column_names_to_snakecase) \\\n",
    "                    .apply(lambda df: column_names_renamed(df, primary_key, stg_primary_key)) \\\n",
    "                    .load_into(table_full_name)\n",
    "                \n",
    "                log.movements = data_loader.df.count()\n",
    "\n",
    "            except Exception as e:\n",
    "                log.error = repr(e)\n",
    "                processing_has_failed = True\n",
    "        \n",
    "            logs.append(log)\n",
    "\n",
    "logger = Logger(f\"{ctr_catalog_name}.loading.data_ingestion_log\")\n",
    "\n",
    "logger.log(logs)\n",
    "\n",
    "if processing_has_failed:\n",
    "    raise RuntimeError(\"Processing of one or more tables has failed. Check the data ingestion log for further info.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
